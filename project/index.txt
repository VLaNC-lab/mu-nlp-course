1:"$Sreact.fragment"
2:I[27423,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js"],"ThemeProvider"]
3:I[3020,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js"],"SiteHeader"]
4:I[39756,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
5:I[37457,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
6:I[22016,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js"],""]
d:I[68027,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
:HL["/mu-nlp-course/_next/static/chunks/d71c64359b385b95.css","style"]
:HL["/mu-nlp-course/_next/static/chunks/ca1fe7bedf895703.css","style"]
:HL["/mu-nlp-course/_next/static/media/0c89a48fa5027cee-s.p.4564287c.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/mu-nlp-course/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/mu-nlp-course/_next/static/media/e8f2fbee2754df70-s.p.9b7a96b4.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"krqxzPtzDoPpo7ON6H005","c":["","project",""],"q":"","i":false,"f":[[["",{"children":["project",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/mu-nlp-course/_next/static/chunks/d71c64359b385b95.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/mu-nlp-course/_next/static/chunks/ca1fe7bedf895703.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","async":true,"nonce":"$undefined"}],["$","script","script-3",{"src":"/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","async":true,"nonce":"$undefined"}],["$","script","script-4",{"src":"/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$","link",null,{"rel":"icon","href":"/mu-nlp-course/favicon.svg","type":"image/svg+xml"}]}],["$","body",null,{"className":"montserrat_4643e776-module__9X2SPa__variable geist_mono_6ed36146-module__KdPPOG__variable space_grotesk_3fa80cbd-module__DWJMMa__variable antialiased","children":["$","$L2",null,{"attribute":"class","defaultTheme":"light","enableSystem":true,"children":[["$","div",null,{"className":"flex min-h-dvh flex-col","children":[["$","$L3",null,{}],["$","main",null,{"className":"flex-1","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"relative overflow-hidden border-t border-white/10 bg-section","children":[["$","div",null,{"className":"pointer-events-none absolute inset-0 bg-hero-grid opacity-20"}],["$","div",null,{"className":"relative mx-auto flex max-w-6xl flex-col gap-4 px-4 py-10 sm:flex-row sm:items-center sm:justify-between","children":[["$","div",null,{"className":"text-muted-foreground text-sm","children":[["$","span",null,{"className":"font-display font-medium text-foreground","children":"VLaNC Lab"}],["$","span",null,{"className":"mx-2","children":"·"}],["$","span",null,{"children":"Vision, Language and Neural Cognition Lab"}]]}],["$","div",null,{"className":"flex flex-wrap items-center gap-x-4 gap-y-2 text-sm","children":[["$","$L6","/",{"href":"/","className":"text-muted-foreground hover:text-foreground","children":"Home"}],["$","$L6","/teachings",{"href":"/teachings","className":"text-muted-foreground hover:text-foreground","children":"Teachings"}],["$","$L6","/books",{"href":"/books","className":"text-muted-foreground hover:text-foreground","children":"Books"}],["$","$L6","/publications",{"href":"/publications","className":"text-muted-foreground hover:text-foreground","children":"Publications"}],["$","$L6","/news",{"href":"/news","className":"text-muted-foreground hover:text-foreground","children":"News"}],["$","$L6","/project",{"href":"/project","className":"text-muted-foreground hover:text-foreground","children":"Project"}],"$L7","$L8"]}]]}]]}]]}],"$L9"]}]}]]}]]}],{"children":["$La",{"children":["$Lb",{},null,false,false]},null,false,false]},null,false,false],"$Lc",false]],"m":"$undefined","G":["$d",[]],"S":true}
e:I[13354,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js"],"Toaster"]
f:I[83767,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"MotionStagger"]
10:I[83767,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"MotionItem"]
11:I[83767,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"MotionGrid"]
1a:I[97367,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"ViewportBoundary"]
1c:I[97367,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"MetadataBoundary"]
1d:"$Sreact.suspense"
7:["$","$L6","/team",{"href":"/team","className":"text-muted-foreground hover:text-foreground","children":"People"}]
8:["$","$L6","/forms",{"href":"/forms","className":"text-muted-foreground hover:text-foreground","children":"Contact"}]
9:["$","$Le",null,{"richColors":true}]
a:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
12:T43e,The application of graph neural networks (GNNs) in predicting missing attributes of products on e-commerce platforms addresses a pertinent challenge in online retail. Traditional methods like LLMs often struggle with training efficiency and fail to adequately capture the intricate relationships between various entities in product catalogs. GNNs, on the other hand, excel in modeling complex relational data structures by directly incorporating graph structures into their architecture. By leveraging the inherent connections between products, attributes, and user interactions, GNNs offer a more effective approach to predict missing attributes while reducing the computational overhead associated with training. Additionally, their ability to embed relationships between entities facilitates more accurate predictions and enhances the overall user experience on e-commerce platforms. This approach not only improves prediction accuracy but also mitigates the carbon footprint associated with inefficient training methods, aligning with sustainability goals in technology development.b:["$","$1","c",{"children":[["$","section",null,{"className":"relative overflow-hidden py-12","children":[["$","div",null,{"className":"pointer-events-none absolute inset-0 bg-section","children":[["$","div",null,{"className":"hero-orb-1 absolute -left-24 top-10 h-[260px] w-[260px] rounded-full blur-3xl opacity-60"}],["$","div",null,{"className":"hero-orb-2 absolute right-[-12%] top-6 h-[280px] w-[280px] rounded-full blur-3xl opacity-50"}],["$","div",null,{"className":"hero-orb-3 absolute bottom-[-20%] left-[35%] h-[280px] w-[280px] rounded-full blur-3xl opacity-55"}],["$","div",null,{"className":"absolute inset-0 bg-hero-grid opacity-30"}]]}],["$","$Lf",null,{"className":"relative mx-auto w-full px-4 max-w-6xl","children":[["$","$L10",null,{"children":["$","div",null,{"className":"glass-panel rounded-3xl border border-white/10 p-6 md:p-8","children":[["$","p",null,{"className":"text-xs uppercase tracking-[0.32em] text-foreground/60","children":"VLaNC Lab"}],["$","h1",null,{"className":"font-display text-[clamp(2.2rem,4vw,3.3rem)] leading-[1.08] tracking-tight","children":["$","span",null,{"className":"text-gradient","children":"Projects"}]}],["$","p",null,{"className":"text-muted-foreground mt-3 max-w-3xl text-base leading-relaxed","children":"A snapshot of the lab's ongoing research themes. Click “Read more” for details."}],null]}]}],["$","$L10",null,{"className":"mt-8","children":["$","$L11",null,{"className":"grid gap-6 md:grid-cols-2","children":[["$","div","attribute-prediction",{"data-slot":"card","className":"glass-card text-card-foreground flex flex-col gap-6 rounded-2xl border border-white/10 py-6 shadow-xl shadow-black/10 transition-all duration-300 hover:-translate-y-1 hover:shadow-2xl h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b border-white/10","children":[["$","div",null,{"data-slot":"card-title","className":"font-display font-semibold tracking-tight text-base","children":"Attribute Prediction in E-commerce"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"attribute prediction"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/Graph_rep_bordered.webp","alt":"Attribute Prediction in E-commerce","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"$12"}]]}],"$L13"]}],"$L14","$L15","$L16"]}]}]]}]]}],["$L17","$L18"],"$L19"]}]
c:["$","$1","h",{"children":[null,["$","$L1a",null,{"children":"$L1b"}],["$","div",null,{"hidden":true,"children":["$","$L1c",null,{"children":["$","$1d",null,{"name":"Next.Metadata","children":"$L1e"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
1f:I[76639,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"Dialog"]
20:I[76639,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"DialogTrigger"]
21:I[76639,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"DialogContent"]
22:I[76639,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"DialogHeader"]
23:I[76639,["/mu-nlp-course/_next/static/chunks/e05ae6051325f235.js","/mu-nlp-course/_next/static/chunks/195c216f52dae3f8.js","/mu-nlp-course/_next/static/chunks/a8820edf395ed42f.js","/mu-nlp-course/_next/static/chunks/b071b927eaf0fd0b.js","/mu-nlp-course/_next/static/chunks/63e3a8df4a95a078.js","/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js"],"DialogTitle"]
27:I[97367,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"OutletBoundary"]
24:T43e,The application of graph neural networks (GNNs) in predicting missing attributes of products on e-commerce platforms addresses a pertinent challenge in online retail. Traditional methods like LLMs often struggle with training efficiency and fail to adequately capture the intricate relationships between various entities in product catalogs. GNNs, on the other hand, excel in modeling complex relational data structures by directly incorporating graph structures into their architecture. By leveraging the inherent connections between products, attributes, and user interactions, GNNs offer a more effective approach to predict missing attributes while reducing the computational overhead associated with training. Additionally, their ability to embed relationships between entities facilitates more accurate predictions and enhances the overall user experience on e-commerce platforms. This approach not only improves prediction accuracy but also mitigates the carbon footprint associated with inefficient training methods, aligning with sustainability goals in technology development.13:["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive glass-chip border-white/30 text-foreground hover:text-foreground/90 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"glass-panel max-w-2xl border border-white/10","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Attribute Prediction in E-commerce"}]}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/Graph_rep_bordered.webp","alt":"Attribute Prediction in E-commerce","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"$24"}]]}]]}]]}]}]
14:["$","div","explainable-ai",{"data-slot":"card","className":"glass-card text-card-foreground flex flex-col gap-6 rounded-2xl border border-white/10 py-6 shadow-xl shadow-black/10 transition-all duration-300 hover:-translate-y-1 hover:shadow-2xl h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b border-white/10","children":[["$","div",null,{"data-slot":"card-title","className":"font-display font-semibold tracking-tight text-base","children":"Explainable AI"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"explainable ai"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/Explainability.webp","alt":"Explainable AI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"While explainable methods have made strides in enhancing transparency in computer vision tasks, the realm of face recognition remains notably devoid of such advancements. Understanding the decision-making process of face recognition models is critical for bolstering their robustness and trustworthiness. Without insights into how these models arrive at their conclusions, there's a risk of biases, errors, and misclassifications going unnoticed. By enforcing more explainability in face recognition systems, researchers aim not only to shed light on their inner workings but also to improve representation learning. Through elucidating the factors influencing recognition decisions, such as facial features, pose variations, or lighting conditions, we can refine model architectures and training strategies to enhance performance and generalization. This pursuit underscores a broader quest to ensure that AI systems operate transparently and ethically, particularly in sensitive domains like biometric identification."}]]}],["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive glass-chip border-white/30 text-foreground hover:text-foreground/90 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"glass-panel max-w-2xl border border-white/10","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Explainable AI"}]}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/Explainability.webp","alt":"Explainable AI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"While explainable methods have made strides in enhancing transparency in computer vision tasks, the realm of face recognition remains notably devoid of such advancements. Understanding the decision-making process of face recognition models is critical for bolstering their robustness and trustworthiness. Without insights into how these models arrive at their conclusions, there's a risk of biases, errors, and misclassifications going unnoticed. By enforcing more explainability in face recognition systems, researchers aim not only to shed light on their inner workings but also to improve representation learning. Through elucidating the factors influencing recognition decisions, such as facial features, pose variations, or lighting conditions, we can refine model architectures and training strategies to enhance performance and generalization. This pursuit underscores a broader quest to ensure that AI systems operate transparently and ethically, particularly in sensitive domains like biometric identification."}]]}]]}]]}]}]]}]
15:["$","div","brain-decoding-bci",{"data-slot":"card","className":"glass-card text-card-foreground flex flex-col gap-6 rounded-2xl border border-white/10 py-6 shadow-xl shadow-black/10 transition-all duration-300 hover:-translate-y-1 hover:shadow-2xl h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b border-white/10","children":[["$","div",null,{"data-slot":"card-title","className":"font-display font-semibold tracking-tight text-base","children":"Brain Decoding and BCI"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"brain decoding bci"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/bci.webp","alt":"Brain Decoding and BCI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"Deep learning has emerged as a powerful tool in decoding brain signals and advancing brain-computer interfacing (BCI) technologies. By leveraging sophisticated neural network architectures, deep learning models can analyze complex patterns within electroencephalography (EEG) or functional magnetic resonance imaging (fMRI) data, enabling the translation of brain activity into actionable commands or insights. Through BCI, individuals can control external devices or interfaces directly with their thoughts, offering unprecedented opportunities for neurorehabilitation, communication assistance for individuals with severe disabilities, and even enhancing human-computer interaction paradigms. The integration of deep learning in this domain continues to push the boundaries of what's possible, holding immense promise for both clinical applications and understanding the intricacies of the human brain."}]]}],["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive glass-chip border-white/30 text-foreground hover:text-foreground/90 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"glass-panel max-w-2xl border border-white/10","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Brain Decoding and BCI"}]}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/bci.webp","alt":"Brain Decoding and BCI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"Deep learning has emerged as a powerful tool in decoding brain signals and advancing brain-computer interfacing (BCI) technologies. By leveraging sophisticated neural network architectures, deep learning models can analyze complex patterns within electroencephalography (EEG) or functional magnetic resonance imaging (fMRI) data, enabling the translation of brain activity into actionable commands or insights. Through BCI, individuals can control external devices or interfaces directly with their thoughts, offering unprecedented opportunities for neurorehabilitation, communication assistance for individuals with severe disabilities, and even enhancing human-computer interaction paradigms. The integration of deep learning in this domain continues to push the boundaries of what's possible, holding immense promise for both clinical applications and understanding the intricacies of the human brain."}]]}]]}]]}]}]]}]
25:T46d,Video moment retrieval, leveraging the latest advancements in deep learning, presents promising avenues for efficiently accessing specific segments within large video archives. Contemporary methods typically employ deep neural networks to learn discriminative features from both visual and temporal cues, enabling accurate moment retrieval based on user queries. However, despite the progress, several challenges persist. One significant issue lies in the scalability and efficiency of these models, particularly when dealing with extensive video datasets. The computational burden associated with processing and analyzing such vast amounts of data can hinder real-time performance and scalability. Additionally, achieving robustness to various visual and temporal variations remains a considerable challenge. Factors such as illumination changes, occlusions, and complex motion patterns can significantly impact retrieval accuracy. Addressing these challenges is paramount for advancing video moment retrieval systems and unlocking their full potential in applications ranging from video summarization to content-based video search.16:["$","div","video-moment-retrieval",{"data-slot":"card","className":"glass-card text-card-foreground flex flex-col gap-6 rounded-2xl border border-white/10 py-6 shadow-xl shadow-black/10 transition-all duration-300 hover:-translate-y-1 hover:shadow-2xl h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b border-white/10","children":[["$","div",null,{"data-slot":"card-title","className":"font-display font-semibold tracking-tight text-base","children":"Video Moment Retrieval"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"video moment retrieval"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/video_moment.webp","alt":"Video Moment Retrieval","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"$25"}]]}],["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive glass-chip border-white/30 text-foreground hover:text-foreground/90 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"glass-panel max-w-2xl border border-white/10","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Video Moment Retrieval"}]}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"glass-panel overflow-hidden rounded-2xl","children":["$","img",null,{"src":"/mu-nlp-course/images/video_moment.webp","alt":"Video Moment Retrieval","className":"h-auto w-full object-cover","loading":"lazy"}]}],"$L26"]}]]}]]}]}]]}]
17:["$","script","script-0",{"src":"/mu-nlp-course/_next/static/chunks/489c5ab4dee2c35b.js","async":true,"nonce":"$undefined"}]
18:["$","script","script-1",{"src":"/mu-nlp-course/_next/static/chunks/ea0f9ac367cb3639.js","async":true,"nonce":"$undefined"}]
19:["$","$L27",null,{"children":["$","$1d",null,{"name":"Next.MetadataOutlet","children":"$@28"}]}]
29:T46d,Video moment retrieval, leveraging the latest advancements in deep learning, presents promising avenues for efficiently accessing specific segments within large video archives. Contemporary methods typically employ deep neural networks to learn discriminative features from both visual and temporal cues, enabling accurate moment retrieval based on user queries. However, despite the progress, several challenges persist. One significant issue lies in the scalability and efficiency of these models, particularly when dealing with extensive video datasets. The computational burden associated with processing and analyzing such vast amounts of data can hinder real-time performance and scalability. Additionally, achieving robustness to various visual and temporal variations remains a considerable challenge. Factors such as illumination changes, occlusions, and complex motion patterns can significantly impact retrieval accuracy. Addressing these challenges is paramount for advancing video moment retrieval systems and unlocking their full potential in applications ranging from video summarization to content-based video search.26:["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"$29"}]
1b:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1e:[["$","title","0",{"children":"Projects"}],["$","meta","1",{"name":"description","content":"Research themes and ongoing projects in the lab."}]]
28:null
