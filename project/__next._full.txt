1:"$Sreact.fragment"
2:I[27423,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"ThemeProvider"]
3:I[22016,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],""]
10:I[68027,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
:HL["/mu-nlp-course/_next/static/chunks/dffeff36c98fe779.css","style"]
:HL["/mu-nlp-course/_next/static/chunks/d9233bc61d31e03e.css","style"]
:HL["/mu-nlp-course/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/mu-nlp-course/_next/static/media/e8f2fbee2754df70-s.p.9b7a96b4.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/logo.webp","image"]
0:{"P":null,"b":"7f8oxV5WJFPgH4BHW5Ihh","c":["","project",""],"q":"","i":false,"f":[[["",{"children":["project",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/mu-nlp-course/_next/static/chunks/dffeff36c98fe779.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/mu-nlp-course/_next/static/chunks/d9233bc61d31e03e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}]}],["$","body",null,{"className":"montserrat_4643e776-module__9X2SPa__variable geist_mono_6ed36146-module__KdPPOG__variable antialiased","children":["$","$L2",null,{"attribute":"class","defaultTheme":"light","enableSystem":true,"children":[["$","div",null,{"className":"flex min-h-dvh flex-col","children":[["$","header",null,{"className":"bg-background/80 supports-[backdrop-filter]:bg-background/60 sticky top-0 z-50 w-full border-b backdrop-blur","children":["$","div",null,{"className":"mx-auto flex h-14 max-w-6xl items-center justify-between gap-4 px-4","children":[["$","$L3",null,{"href":"/","className":"flex items-center gap-2 font-semibold","children":[["$","img",null,{"src":"/logo.webp","alt":"VLaNC Lab","className":"h-7 w-auto"}],["$","span",null,{"className":"hidden sm:inline","children":"VLaNC Lab"}]]}],["$","nav",null,{"className":"hidden items-center gap-1 md:flex","children":[["$","$L3","/",{"href":"/","children":"Home","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}],["$","$L3","/teachings",{"href":"/teachings","children":"Teachings","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}],["$","$L3","/books",{"href":"/books","children":"Books","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}],"$L4","$L5","$L6","$L7","$L8"]}],"$L9"]}]}],"$La","$Lb"]}],"$Lc"]}]}]]}]]}],{"children":["$Ld",{"children":["$Le",{},null,false,false]},null,false,false]},null,false,false],"$Lf",false]],"m":"$undefined","G":["$10",[]],"S":true}
11:I[80376,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"Sheet"]
12:I[80376,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"SheetTrigger"]
13:I[80376,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"SheetContent"]
14:I[80376,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"SheetHeader"]
15:I[80376,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"SheetTitle"]
16:I[80376,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"SheetClose"]
1b:I[39756,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
1c:I[37457,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
1d:I[13354,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js"],"Toaster"]
1f:I[76639,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js","/mu-nlp-course/_next/static/chunks/290ee996a3e14f70.js"],"Dialog"]
20:I[76639,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js","/mu-nlp-course/_next/static/chunks/290ee996a3e14f70.js"],"DialogTrigger"]
21:I[76639,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js","/mu-nlp-course/_next/static/chunks/290ee996a3e14f70.js"],"DialogContent"]
22:I[76639,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js","/mu-nlp-course/_next/static/chunks/290ee996a3e14f70.js"],"DialogHeader"]
23:I[76639,["/mu-nlp-course/_next/static/chunks/f9fa86c1e5829e90.js","/mu-nlp-course/_next/static/chunks/f721aa8f63c186a3.js","/mu-nlp-course/_next/static/chunks/1bb79dbbd8943828.js","/mu-nlp-course/_next/static/chunks/290ee996a3e14f70.js"],"DialogTitle"]
2a:I[97367,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"ViewportBoundary"]
2c:I[97367,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"MetadataBoundary"]
2d:"$Sreact.suspense"
4:["$","$L3","/publications",{"href":"/publications","children":"Publications","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}]
5:["$","$L3","/news",{"href":"/news","children":"News","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}]
6:["$","$L3","/project",{"href":"/project","children":"Project","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}]
7:["$","$L3","/team",{"href":"/team","children":"People","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}]
8:["$","$L3","/forms",{"href":"/forms","children":"Contact","data-slot":"button","data-variant":"ghost","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","ref":null}]
9:["$","div",null,{"className":"md:hidden","children":["$","$L11",null,{"children":[["$","$L12",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"ghost","data-size":"icon","className":"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 size-9","aria-label":"Open menu","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-menu size-5","aria-hidden":"true","children":[["$","path","1tepv9",{"d":"M4 5h16"}],["$","path","1lakjw",{"d":"M4 12h16"}],["$","path","1djgab",{"d":"M4 19h16"}],"$undefined"]}]}]}],["$","$L13",null,{"side":"right","className":"p-0","children":[["$","$L14",null,{"className":"border-b","children":["$","$L15",null,{"children":"Menu"}]}],["$","div",null,{"className":"flex flex-col gap-1 p-4","children":[["$","$L16","/",{"asChild":true,"children":["$","$L3",null,{"href":"/","children":"Home","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}],["$","$L16","/teachings",{"asChild":true,"children":["$","$L3",null,{"href":"/teachings","children":"Teachings","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}],["$","$L16","/books",{"asChild":true,"children":["$","$L3",null,{"href":"/books","children":"Books","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}],["$","$L16","/publications",{"asChild":true,"children":["$","$L3",null,{"href":"/publications","children":"Publications","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}],"$L17","$L18","$L19","$L1a"]}]]}]]}]}]
a:["$","main",null,{"className":"flex-1","children":["$","$L1b",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L1c",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]
b:["$","footer",null,{"className":"border-t","children":["$","div",null,{"className":"mx-auto flex max-w-6xl flex-col gap-4 px-4 py-10 sm:flex-row sm:items-center sm:justify-between","children":[["$","div",null,{"className":"text-muted-foreground text-sm","children":[["$","span",null,{"className":"font-medium text-foreground","children":"VLaNC Lab"}],["$","span",null,{"className":"mx-2","children":"·"}],["$","span",null,{"children":"Vision, Language and Neural Cognition Lab"}]]}],["$","div",null,{"className":"flex flex-wrap items-center gap-x-4 gap-y-2 text-sm","children":[["$","$L3","/",{"href":"/","className":"text-muted-foreground hover:text-foreground","children":"Home"}],["$","$L3","/teachings",{"href":"/teachings","className":"text-muted-foreground hover:text-foreground","children":"Teachings"}],["$","$L3","/books",{"href":"/books","className":"text-muted-foreground hover:text-foreground","children":"Books"}],["$","$L3","/publications",{"href":"/publications","className":"text-muted-foreground hover:text-foreground","children":"Publications"}],["$","$L3","/news",{"href":"/news","className":"text-muted-foreground hover:text-foreground","children":"News"}],["$","$L3","/project",{"href":"/project","className":"text-muted-foreground hover:text-foreground","children":"Project"}],["$","$L3","/team",{"href":"/team","className":"text-muted-foreground hover:text-foreground","children":"People"}],["$","$L3","/forms",{"href":"/forms","className":"text-muted-foreground hover:text-foreground","children":"Contact"}]]}]]}]}]
c:["$","$L1d",null,{"richColors":true}]
d:["$","$1","c",{"children":[null,["$","$L1b",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L1c",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
1e:T43e,The application of graph neural networks (GNNs) in predicting missing attributes of products on e-commerce platforms addresses a pertinent challenge in online retail. Traditional methods like LLMs often struggle with training efficiency and fail to adequately capture the intricate relationships between various entities in product catalogs. GNNs, on the other hand, excel in modeling complex relational data structures by directly incorporating graph structures into their architecture. By leveraging the inherent connections between products, attributes, and user interactions, GNNs offer a more effective approach to predict missing attributes while reducing the computational overhead associated with training. Additionally, their ability to embed relationships between entities facilitates more accurate predictions and enhances the overall user experience on e-commerce platforms. This approach not only improves prediction accuracy but also mitigates the carbon footprint associated with inefficient training methods, aligning with sustainability goals in technology development.e:["$","$1","c",{"children":[["$","div",null,{"className":"mx-auto max-w-6xl px-4 py-10","children":[["$","div",null,{"className":"space-y-2","children":[["$","h1",null,{"className":"text-h1 font-semibold tracking-tight","children":"Projects"}],["$","p",null,{"className":"text-muted-foreground max-w-2xl leading-relaxed","children":"A snapshot of the lab's ongoing research themes. Click “Read more” for details."}]]}],["$","div",null,{"className":"mt-8 grid gap-6 md:grid-cols-2","children":[["$","div","attribute-prediction",{"data-slot":"card","className":"bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b","children":[["$","div",null,{"data-slot":"card-title","className":"font-semibold text-base","children":"Attribute Prediction in E-commerce"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"attribute prediction"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/Graph_rep_bordered.webp","alt":"Attribute Prediction in E-commerce","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"$1e"}]]}],["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"max-w-2xl","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Attribute Prediction in E-commerce"}]}],"$L24"]}]]}]}]]}],"$L25","$L26","$L27"]}]]}],["$L28"],"$L29"]}]
f:["$","$1","h",{"children":[null,["$","$L2a",null,{"children":"$L2b"}],["$","div",null,{"hidden":true,"children":["$","$L2c",null,{"children":["$","$2d",null,{"name":"Next.Metadata","children":"$L2e"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
32:I[97367,["/mu-nlp-course/_next/static/chunks/ff1a16fafef87110.js","/mu-nlp-course/_next/static/chunks/d2be314c3ece3fbe.js"],"OutletBoundary"]
17:["$","$L16","/news",{"asChild":true,"children":["$","$L3",null,{"href":"/news","children":"News","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}]
18:["$","$L16","/project",{"asChild":true,"children":["$","$L3",null,{"href":"/project","children":"Project","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}]
19:["$","$L16","/team",{"asChild":true,"children":["$","$L3",null,{"href":"/team","children":"People","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}]
1a:["$","$L16","/forms",{"asChild":true,"children":["$","$L3",null,{"href":"/forms","children":"Contact","data-slot":"button","data-variant":"ghost","data-size":"default","className":"inline-flex items-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50 h-9 px-4 py-2 has-[>svg]:px-3 justify-start","ref":null}]}]
2f:T43e,The application of graph neural networks (GNNs) in predicting missing attributes of products on e-commerce platforms addresses a pertinent challenge in online retail. Traditional methods like LLMs often struggle with training efficiency and fail to adequately capture the intricate relationships between various entities in product catalogs. GNNs, on the other hand, excel in modeling complex relational data structures by directly incorporating graph structures into their architecture. By leveraging the inherent connections between products, attributes, and user interactions, GNNs offer a more effective approach to predict missing attributes while reducing the computational overhead associated with training. Additionally, their ability to embed relationships between entities facilitates more accurate predictions and enhances the overall user experience on e-commerce platforms. This approach not only improves prediction accuracy but also mitigates the carbon footprint associated with inefficient training methods, aligning with sustainability goals in technology development.24:["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/Graph_rep_bordered.webp","alt":"Attribute Prediction in E-commerce","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"$2f"}]]}]
25:["$","div","explainable-ai",{"data-slot":"card","className":"bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b","children":[["$","div",null,{"data-slot":"card-title","className":"font-semibold text-base","children":"Explainable AI"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"explainable ai"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/Explainability.webp","alt":"Explainable AI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"While explainable methods have made strides in enhancing transparency in computer vision tasks, the realm of face recognition remains notably devoid of such advancements. Understanding the decision-making process of face recognition models is critical for bolstering their robustness and trustworthiness. Without insights into how these models arrive at their conclusions, there's a risk of biases, errors, and misclassifications going unnoticed. By enforcing more explainability in face recognition systems, researchers aim not only to shed light on their inner workings but also to improve representation learning. Through elucidating the factors influencing recognition decisions, such as facial features, pose variations, or lighting conditions, we can refine model architectures and training strategies to enhance performance and generalization. This pursuit underscores a broader quest to ensure that AI systems operate transparently and ethically, particularly in sensitive domains like biometric identification."}]]}],["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"max-w-2xl","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Explainable AI"}]}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/Explainability.webp","alt":"Explainable AI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"While explainable methods have made strides in enhancing transparency in computer vision tasks, the realm of face recognition remains notably devoid of such advancements. Understanding the decision-making process of face recognition models is critical for bolstering their robustness and trustworthiness. Without insights into how these models arrive at their conclusions, there's a risk of biases, errors, and misclassifications going unnoticed. By enforcing more explainability in face recognition systems, researchers aim not only to shed light on their inner workings but also to improve representation learning. Through elucidating the factors influencing recognition decisions, such as facial features, pose variations, or lighting conditions, we can refine model architectures and training strategies to enhance performance and generalization. This pursuit underscores a broader quest to ensure that AI systems operate transparently and ethically, particularly in sensitive domains like biometric identification."}]]}]]}]]}]}]]}]
26:["$","div","brain-decoding-bci",{"data-slot":"card","className":"bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b","children":[["$","div",null,{"data-slot":"card-title","className":"font-semibold text-base","children":"Brain Decoding and BCI"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"brain decoding bci"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/bci.webp","alt":"Brain Decoding and BCI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"Deep learning has emerged as a powerful tool in decoding brain signals and advancing brain-computer interfacing (BCI) technologies. By leveraging sophisticated neural network architectures, deep learning models can analyze complex patterns within electroencephalography (EEG) or functional magnetic resonance imaging (fMRI) data, enabling the translation of brain activity into actionable commands or insights. Through BCI, individuals can control external devices or interfaces directly with their thoughts, offering unprecedented opportunities for neurorehabilitation, communication assistance for individuals with severe disabilities, and even enhancing human-computer interaction paradigms. The integration of deep learning in this domain continues to push the boundaries of what's possible, holding immense promise for both clinical applications and understanding the intricacies of the human brain."}]]}],["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"max-w-2xl","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Brain Decoding and BCI"}]}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/bci.webp","alt":"Brain Decoding and BCI","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"Deep learning has emerged as a powerful tool in decoding brain signals and advancing brain-computer interfacing (BCI) technologies. By leveraging sophisticated neural network architectures, deep learning models can analyze complex patterns within electroencephalography (EEG) or functional magnetic resonance imaging (fMRI) data, enabling the translation of brain activity into actionable commands or insights. Through BCI, individuals can control external devices or interfaces directly with their thoughts, offering unprecedented opportunities for neurorehabilitation, communication assistance for individuals with severe disabilities, and even enhancing human-computer interaction paradigms. The integration of deep learning in this domain continues to push the boundaries of what's possible, holding immense promise for both clinical applications and understanding the intricacies of the human brain."}]]}]]}]]}]}]]}]
30:T46d,Video moment retrieval, leveraging the latest advancements in deep learning, presents promising avenues for efficiently accessing specific segments within large video archives. Contemporary methods typically employ deep neural networks to learn discriminative features from both visual and temporal cues, enabling accurate moment retrieval based on user queries. However, despite the progress, several challenges persist. One significant issue lies in the scalability and efficiency of these models, particularly when dealing with extensive video datasets. The computational burden associated with processing and analyzing such vast amounts of data can hinder real-time performance and scalability. Additionally, achieving robustness to various visual and temporal variations remains a considerable challenge. Factors such as illumination changes, occlusions, and complex motion patterns can significantly impact retrieval accuracy. Addressing these challenges is paramount for advancing video moment retrieval systems and unlocking their full potential in applications ranging from video summarization to content-based video search.31:T46d,Video moment retrieval, leveraging the latest advancements in deep learning, presents promising avenues for efficiently accessing specific segments within large video archives. Contemporary methods typically employ deep neural networks to learn discriminative features from both visual and temporal cues, enabling accurate moment retrieval based on user queries. However, despite the progress, several challenges persist. One significant issue lies in the scalability and efficiency of these models, particularly when dealing with extensive video datasets. The computational burden associated with processing and analyzing such vast amounts of data can hinder real-time performance and scalability. Additionally, achieving robustness to various visual and temporal variations remains a considerable challenge. Factors such as illumination changes, occlusions, and complex motion patterns can significantly impact retrieval accuracy. Addressing these challenges is paramount for advancing video moment retrieval systems and unlocking their full potential in applications ranging from video summarization to content-based video search.27:["$","div","video-moment-retrieval",{"data-slot":"card","className":"bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm h-full","children":[["$","div",null,{"data-slot":"card-header","className":"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6 border-b","children":[["$","div",null,{"data-slot":"card-title","className":"font-semibold text-base","children":"Video Moment Retrieval"}],["$","div",null,{"data-slot":"card-description","className":"text-muted-foreground text-sm","children":"video moment retrieval"}]]}],["$","div",null,{"data-slot":"card-content","className":"px-6 space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/video_moment.webp","alt":"Video Moment Retrieval","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed line-clamp-6","children":"$30"}]]}],["$","div",null,{"data-slot":"card-footer","className":"flex items-center px-6 [.border-t]:pt-6 justify-end","children":["$","$L1f",null,{"children":[["$","$L20",null,{"asChild":true,"children":["$","button",null,{"data-slot":"button","data-variant":"outline","data-size":"sm","className":"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5","children":"Read more"}]}],["$","$L21",null,{"className":"max-w-2xl","children":[["$","$L22",null,{"children":["$","$L23",null,{"children":"Video Moment Retrieval"}]}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"overflow-hidden rounded-lg border","children":["$","img",null,{"src":"/images/video_moment.webp","alt":"Video Moment Retrieval","className":"h-auto w-full object-cover","loading":"lazy"}]}],["$","p",null,{"className":"text-muted-foreground text-sm leading-relaxed","children":"$31"}]]}]]}]]}]}]]}]
28:["$","script","script-0",{"src":"/mu-nlp-course/_next/static/chunks/290ee996a3e14f70.js","async":true,"nonce":"$undefined"}]
29:["$","$L32",null,{"children":["$","$2d",null,{"name":"Next.MetadataOutlet","children":"$@33"}]}]
2b:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
2e:[["$","title","0",{"children":"Projects"}],["$","meta","1",{"name":"description","content":"Research themes and ongoing projects in the lab."}]]
33:null
